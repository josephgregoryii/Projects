# -*- coding: utf-8 -*-
"""projOPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yhzsmAjogNdJv8aDKUEfnAG6K3ZvHnTf
"""

#Author:      Joseph Gregory
#Project:     Neural Networks

##Tutorial code (copied from tutorial for cleaner reference)
# 3. Import libraries and modules

import numpy as np
np.random.seed(123)  # for reproducibility
 
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
from keras.datasets import mnist
 
# 4. Load pre-shuffled MNIST data into train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()


 
# 5. Preprocess input data
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
 
# 6. Preprocess class labels
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)
 

# 7. Define model architecture
model = Sequential()
 
model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))
model.add(Convolution2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
 
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
 
# 8. Compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
# 9. Fit model on training data
model.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)
 
# 10. Evaluate model on test data
score = model.evaluate(X_test, Y_test, verbose=0)

#model1
model1 = Sequential()


#gotta shape dat data
model1.add(Flatten(input_shape=X_train.shape[1:]))

#16 hidden sigmoid layers
model1.add(Dense(16, activation="sigmoid"))

#0dropout
model1.add(Dropout(0))

#10 output layers with softmax
model1.add(Dense(10, activation="softmax"))

model1.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model1.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

#model2
model2 = Sequential()

#gotta make shape dat data
model2.add(Flatten(input_shape=X_train.shape[1:]))

#16 hidden sigmoid layers
model2.add(Dense(128, activation="sigmoid"))

#0dropout
model2.add(Dropout(0))

#10 output layers with softmax
model2.add(Dense(10, activation="softmax"))

model2.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model2.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

#model3
model3 = Sequential()

#shape dat data
model3.add(Flatten(input_shape=X_train.shape[1:]))

#16 hidden relu layers
model3.add(Dense(128, activation="relu"))

#0 dropout
model3.add(Dropout(0))

#10 output layers with softmax
model3.add(Dense(10, activation="softmax"))

model3.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model3.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

#model4
model4 = Sequential()

#shape dat data
model4.add(Flatten(input_shape=X_train.shape[1:]))

#16 hidden sigmoid layers
model4.add(Dense(128, activation="relu"))

#0.5 dropout
model4.add(Dropout(0.5))

#10 output layers with softmax
model4.add(Dense(10, activation="softmax"))

model4.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model4.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

model5 = Sequential()

model5.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))
model5.add(Flatten())

#fiddy dropout

model5.add(Dense(128, activation="relu"))

model5.add(Dropout(0.5))

#10 output layers with softmax
model5.add(Dense(10, activation="softmax"))

model5.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model5.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

model6 = Sequential()

model6.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))
model6.add(MaxPooling2D(pool_size=(2,2)))
model6.add(Dropout(0.25))
model6.add(Flatten())

#fiddy dropout

model6.add(Dense(128, activation="relu"))

model6.add(Dropout(0.5))

#10 output layers with softmax
model6.add(Dense(10, activation="softmax"))

model6.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model6.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

model7 = Sequential()

model7.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))
model7.add(Convolution2D(32, (3, 3), activation='relu'))
model7.add(MaxPooling2D(pool_size=(2,2)))
model7.add(Dropout(0.25))
model7.add(Flatten())

model7.add(Dense(128, activation="relu"))

model7.add(Dropout(0.5))

#10 output layers with softmax
model7.add(Dense(10, activation="softmax"))

model7.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model7.fit(X_train, Y_train, 
          batch_size=32, epochs=10, verbose=1)

from keras.datasets import cifar10

(X_train2, y_train2), (X_test2, y_test2) = cifar10.load_data()


 
# 5. Preprocess input data
X_train2 = X_train2.reshape(X_train2.shape[0], 3, 32, 32)
X_test2 = X_test2.reshape(X_test2.shape[0], 3, 32, 32)
X_train2 = X_train2.astype('float32')
X_test2 = X_test2.astype('float32')
X_train2 /= 255
X_test2 /= 255
 
# 6. Preprocess class labels
Y_train2 = np_utils.to_categorical(y_train2, 10)
Y_test2 = np_utils.to_categorical(y_test, 10)

#model8
model8 = Sequential()

#gotta make shape dat data
model8.add(Flatten(input_shape=X_train2.shape[1:]))

#16 hidden relu layers
model8.add(Dense(128, activation="relu"))

#0dropout
model8.add(Dropout(0.5))

#10 output layers with softmax
model8.add(Dense(10, activation="softmax"))

model8.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model8.fit(X_train2, Y_train2, 
          batch_size=32, epochs=10, verbose=1)

#model9
model9 = Sequential()

model9.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(3,32,32), data_format='channels_first'))
model9.add(Convolution2D(32, (3, 3), activation='relu'))
model9.add(MaxPooling2D(pool_size=(2,2)))
model9.add(Dropout(0.25))
model9.add(Flatten())

model9.add(Dense(128, activation="relu"))

model9.add(Dropout(0.5))

#10 output layers with softmax
model9.add(Dense(10, activation="softmax"))

model9.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
model9.fit(X_train2, Y_train2, 
          batch_size=32, epochs=10, verbose=1)

_score1 = model1.evaluate(X_train, Y_train, verbose=0)
_score2 = model2.evaluate(X_train, Y_train, verbose=0)
_score3 = model3.evaluate(X_train, Y_train, verbose=0)
_score4 = model4.evaluate(X_train, Y_train, verbose=0)
_score5 = model5.evaluate(X_train, Y_train, verbose=0)
_score6 = model6.evaluate(X_train, Y_train, verbose=0)
_score7 = model7.evaluate(X_train, Y_train, verbose=0)
_score8 = model8.evaluate(X_train2, Y_train2, verbose=0)
_score9 = model9.evaluate(X_train2, Y_train2, verbose=0)

score1 = model1.evaluate(X_test, Y_test, verbose=0)
score2 = model2.evaluate(X_test, Y_test, verbose=0)
score3 = model3.evaluate(X_test, Y_test, verbose=0)
score4 = model4.evaluate(X_test, Y_test, verbose=0)
score5 = model5.evaluate(X_test, Y_test, verbose=0)
score6 = model6.evaluate(X_test, Y_test, verbose=0)
score7 = model7.evaluate(X_test, Y_test, verbose=0)
score8 = model8.evaluate(X_test2, Y_test2, verbose=0)
score9 = model9.evaluate(X_test2, Y_test2, verbose=0)

print("Accuracy for model1 using training dataset:",_score1[1])
print("Accuracy for model1 using testing dataset:",score1[1],"\n")

print("Accuracy for model2 using training dataset:",_score2[1])
print("Accuracy for model2 using testing dataset:",score2[1],"\n")

print("Accuracy for model3 using training dataset:",_score3[1])
print("Accuracy for model3 using testing dataset:",score3[1],"\n")

print("Accuracy for model4 using training dataset:",_score4[1])
print("Accuracy for model4 using testing dataset:",score4[1],"\n")

print("Accuracy for model5 using training dataset:",_score5[1])
print("Accuracy for model5 using testing dataset:",score5[1],"\n")

print("Accuracy for model6 using training dataset:",_score6[1])
print("Accuracy for model6 using testing dataset:",score6[1],"\n")

print("Accuracy for model7 using training dataset:",_score7[1])
print("Accuracy for model7 using testing dataset:",score7[1],"\n")

print("Accuracy for model8 using training dataset:",_score8[1])
print("Accuracy for model8 using testing dataset:",score8[1],"\n")

print("Accuracy for model9 using training dataset:",_score9[1])
print("Accuracy for model9 using testing dataset:",score9[1],"\n")
